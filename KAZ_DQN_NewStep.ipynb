{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-28 15:59:40.239109: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-28 15:59:40.255200: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-28 15:59:40.275197: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-28 15:59:40.281552: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-28 15:59:40.294550: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-28 15:59:41.298462: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from pettingzoo.butterfly import knights_archers_zombies_v10\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import optuna\n",
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L'environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PettingZooEnvWrapper(gym.Env):\n",
    "    def __init__(self, env):\n",
    "        super(PettingZooEnvWrapper, self).__init__()\n",
    "        self.env = env\n",
    "        self.env.reset()\n",
    "        self.action_space = gym.spaces.Discrete(self.env.action_space(self.env.agents[0]).n)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=1, shape=self.env.observation_space(self.env.agents[0]).shape, dtype=np.float32)\n",
    "        self.current_agent = self.env.agents[0]\n",
    "        self.agents=self.env.agents\n",
    "    def reset(self,seed=None, **kwargs):\n",
    "        self.env.reset(seed=seed)\n",
    "        self.current_agent = self.env.agents[0]\n",
    "        obs, _, _, _,infos = self.env.last()\n",
    "        return obs,infos\n",
    "    def step(self, action):\n",
    "        y_tolerance = 0.98\n",
    "        collision_threshold = 0.02\n",
    "        self.env.step(action)\n",
    "        num_zombies = self.env.env.max_zombies\n",
    "        obs, reward,terminated,truncated, info = self.env.last()\n",
    "        self.current_agent = self.env.agent_selection\n",
    "        #-------------------------------------------------------\n",
    "        # Extract zombie positions and detect collisions for penalties\n",
    "        zombies = obs[-num_zombies:]  # Extract the last rows representing zombies\n",
    "        total_zombies_reached_border = 0\n",
    "        total_collisions = 0\n",
    "\n",
    "        for zombie in zombies:\n",
    "            distance_to_current_agent = zombie[0]  # Distance to the agent\n",
    "            y_position = zombie[1]  # Y-position of the zombie\n",
    "            x_position = zombie[2] # X-position of the zombie\n",
    "        \n",
    "            # Check if the zombie reached the border (y=1)\n",
    "            if y_position >= y_tolerance and x_position != 0 and distance_to_current_agent != 0:\n",
    "                #y_position = 0, x_position = 0, distance_to_current_agent = 0 means the zombie is not spawn or dead\n",
    "                total_zombies_reached_border += 1  # Track zombies that reached the border\n",
    "\n",
    "            # Check if the agent collided with the zombie (close distance)\n",
    "            if distance_to_current_agent <= collision_threshold and x_position != 0 and y_position >= 0:\n",
    "                #y_position = 0, x_position = 0, distance_to_current_agent = 0 means the zombie is not spawn or dead\n",
    "                total_collisions += 1\n",
    "\n",
    "        # Modify the real-time reward during training based on penalties\n",
    "        if total_zombies_reached_border > 0:\n",
    "            reward -= 1  # Penalize the agent for letting zombies reach the border\n",
    "\n",
    "        if total_collisions > 0:\n",
    "            reward -= 1  # Penalize the agent for colliding with a zombie\n",
    "        #-------------------------------------------------------\n",
    "        return obs, reward, terminated,truncated, info\n",
    "    \n",
    "    def render(self):\n",
    "        return self.env.render()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env = knights_archers_zombies_v10.env(render_mode=\"human\",max_zombies=4,max_cycles=100)\n",
    "env = knights_archers_zombies_v10.env(render_mode=\"rgb_array\",max_zombies=4,max_cycles=100)\n",
    "\n",
    "wrapped_env = PettingZooEnvWrapper(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boucle d'apprentisage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, agents, num_episodes=100,affichage=True):\n",
    "    for episode in range(num_episodes):\n",
    "        env.reset()\n",
    "        terminated = False\n",
    "        truncated=False\n",
    "        #print(episode)\n",
    "        for agent in env.agents:\n",
    "            if terminated or truncated:\n",
    "                print(not (terminated or truncated))\n",
    "            current_agent = env.current_agent\n",
    "                \n",
    "            obs, _, terminated,truncated, _ = env.env.last()\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "                \n",
    "            action, _states = agents[agent].predict(obs, deterministic=True)\n",
    "            #print(action)\n",
    "            env.step(action)\n",
    "                \n",
    "            next_obs, reward, terminated,truncated, _ = env.env.last()\n",
    "                \n",
    "            # Collect experience and train the agent\n",
    "            agents[agent].learn(total_timesteps=1000)\n",
    "\n",
    "                \n",
    "        # Optionally evaluate performance\n",
    "        if episode % 10 == 0 and affichage:\n",
    "            for agent in agents:\n",
    "                mean_reward, _ = evaluate_policy(agents[agent], wrapped_env, n_eval_episodes=10)\n",
    "                print(f\"Agent {agent}: Mean reward: {mean_reward}\")\n",
    "\n",
    "    env.env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optuna optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_dqn_params(trial: optuna.Trial) :\n",
    "    \"\"\"\n",
    "    Sampler for DQN hyperparams.\n",
    "\n",
    "    :param trial:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    gamma = trial.suggest_categorical(\"gamma\", [0.9, 0.95, 0.98, 0.99, 0.995, 0.999, 0.9999])\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64, 100, 128, 256, 512])\n",
    "    buffer_size = trial.suggest_categorical(\"buffer_size\", [int(1e4), int(5e4), int(1e5), int(1e6)])\n",
    "    exploration_final_eps = trial.suggest_float(\"exploration_final_eps\", 0, 0.2)\n",
    "    exploration_fraction = trial.suggest_float(\"exploration_fraction\", 0, 0.5)\n",
    "    target_update_interval = trial.suggest_categorical(\"target_update_interval\", [1, 1000, 5000, 10000, 15000, 20000])\n",
    "    learning_starts = trial.suggest_categorical(\"learning_starts\", [0, 1000, 5000, 10000, 20000])\n",
    "\n",
    "    train_freq = trial.suggest_categorical(\"train_freq\", [1, 4, 8, 16, 128, 256, 1000])\n",
    "    subsample_steps = trial.suggest_categorical(\"subsample_steps\", [1, 2, 4, 8])\n",
    "    gradient_steps = max(train_freq // subsample_steps, 1)\n",
    "\n",
    "    net_arch_type = trial.suggest_categorical(\"net_arch\", [\"tiny\", \"small\", \"medium\"])\n",
    "\n",
    "    net_arch = {\"tiny\": [64], \"small\": [64, 64], \"medium\": [256, 256]}[net_arch_type]\n",
    "\n",
    "    hyperparams = {\n",
    "        \"gamma\": gamma,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"buffer_size\": buffer_size,\n",
    "        \"train_freq\": train_freq,\n",
    "        \"gradient_steps\": gradient_steps,\n",
    "        \"exploration_fraction\": exploration_fraction,\n",
    "        \"exploration_final_eps\": exploration_final_eps,\n",
    "        \"target_update_interval\": target_update_interval,\n",
    "        \"learning_starts\": learning_starts,\n",
    "        \"policy_kwargs\": dict(net_arch=net_arch),\n",
    "    }\n",
    "\n",
    "    env = knights_archers_zombies_v10.env(render_mode=\"rgb_array\",max_zombies=4,max_cycles=100)\n",
    "\n",
    "    wrapped_env = PettingZooEnvWrapper(env)\n",
    "    agents = {agent: DQN(\"MlpPolicy\", wrapped_env, verbose=0,**hyperparams) for agent in env.agents}\n",
    "\n",
    "    train(wrapped_env,agents,num_episodes=10,affichage=False)\n",
    "    total_reward=0\n",
    "    for agent in agents:\n",
    "            mean_reward, _ = evaluate_policy(agents[agent], wrapped_env, n_eval_episodes=10)\n",
    "            total_reward+=mean_reward\n",
    "\n",
    "\n",
    "    return -total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_dqn=optuna.study.create_study()\n",
    "study_dqn.optimize(sample_dqn_params, n_trials=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baris\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\stable_baselines3\\common\\buffers.py:241: UserWarning: This system does not have apparently enough memory to store the complete replay buffer 0.86GB > 0.84GB\n",
      "  warnings.warn(\n",
      "C:\\Users\\baris\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\stable_baselines3\\common\\buffers.py:241: UserWarning: This system does not have apparently enough memory to store the complete replay buffer 0.86GB > 0.78GB\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "best_hyperparams={'gamma': 0.95,\n",
    " 'learning_rate': 0.0057832857004646385,\n",
    " 'batch_size': 512,\n",
    " 'buffer_size': 1000000,\n",
    " 'exploration_final_eps': 0.038076390363142434,\n",
    " 'exploration_fraction': 0.1311871993781526,\n",
    " 'target_update_interval': 15000,\n",
    " 'learning_starts': 1000,\n",
    " 'train_freq': 256,\n",
    " \"policy_kwargs\": dict(net_arch=[64])}\n",
    "agents = {agent: DQN(\"MlpPolicy\", wrapped_env, verbose=0,**best_hyperparams) for agent in env.agents}\n",
    "#best_hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baris\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent archer_0: Mean reward: 0.2\n",
      "Agent archer_1: Mean reward: 0.0\n",
      "Agent knight_0: Mean reward: 0.0\n",
      "Agent knight_1: Mean reward: 0.0\n",
      "Agent archer_0: Mean reward: 0.0\n",
      "Agent archer_1: Mean reward: 0.0\n",
      "Agent knight_0: Mean reward: 0.0\n",
      "Agent knight_1: Mean reward: 0.0\n",
      "Agent archer_0: Mean reward: 0.5\n",
      "Agent archer_1: Mean reward: 0.0\n",
      "Agent knight_0: Mean reward: 0.0\n",
      "Agent knight_1: Mean reward: 0.1\n",
      "Agent archer_0: Mean reward: 0.0\n",
      "Agent archer_1: Mean reward: 0.0\n",
      "Agent knight_0: Mean reward: 0.1\n",
      "Agent knight_1: Mean reward: 0.2\n",
      "Agent archer_0: Mean reward: 0.0\n",
      "Agent archer_1: Mean reward: 0.0\n",
      "Agent knight_0: Mean reward: 0.0\n",
      "Agent knight_1: Mean reward: 0.0\n",
      "Agent archer_0: Mean reward: 0.7\n",
      "Agent archer_1: Mean reward: 0.0\n",
      "Agent knight_0: Mean reward: 0.0\n",
      "Agent knight_1: Mean reward: 0.0\n",
      "Agent archer_0: Mean reward: 0.1\n",
      "Agent archer_1: Mean reward: 0.0\n",
      "Agent knight_0: Mean reward: 0.0\n",
      "Agent knight_1: Mean reward: 0.7\n",
      "Agent archer_0: Mean reward: 0.5\n",
      "Agent archer_1: Mean reward: 0.3\n",
      "Agent knight_0: Mean reward: 0.2\n",
      "Agent knight_1: Mean reward: 0.0\n",
      "Agent archer_0: Mean reward: 0.0\n",
      "Agent archer_1: Mean reward: 0.0\n",
      "Agent knight_0: Mean reward: 0.1\n",
      "Agent knight_1: Mean reward: 0.0\n",
      "Agent archer_0: Mean reward: 0.0\n",
      "Agent archer_1: Mean reward: 0.0\n",
      "Agent knight_0: Mean reward: 0.5\n",
      "Agent knight_1: Mean reward: 0.5\n"
     ]
    }
   ],
   "source": [
    "train(wrapped_env, agents, 100) #10 pas d'apprentissage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affichage de l'environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_trained_agents(env, agents, max_steps_per_episode=100):\n",
    "    obs, _ = env.reset()  # Reset environment\n",
    "    step_count = 0  # Initialize step counter\n",
    "\n",
    "    while step_count < max_steps_per_episode:\n",
    "        current_agent = env.current_agent\n",
    "\n",
    "        # Get current observation\n",
    "        obs, _, _, _, _ = env.env.last()\n",
    "\n",
    "        # Get action from the trained agent's policy\n",
    "        action, _states = agents[current_agent].predict(obs, deterministic=True)\n",
    "\n",
    "        # Step through the environment\n",
    "        next_obs, reward, _, _, _ = env.step(action)\n",
    "\n",
    "        # Render the environment\n",
    "        env.render()\n",
    "\n",
    "        # Increment the step counter\n",
    "        step_count += 1\n",
    "\n",
    "    env.env.close()  # Close the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_env = knights_archers_zombies_v10.env(render_mode=\"human\",max_zombies=4,max_cycles=100)\n",
    "\n",
    "human_wrapped_env = PettingZooEnvWrapper(human_env)\n",
    "render_trained_agents(human_wrapped_env, agents, max_steps_per_episode=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot de performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_logging(env, agents, num_episodes=10000):\n",
    "    agent_rewards = {agent: [] for agent in env.agents}  # Store cumulative rewards for each agent\n",
    "    mean_rewards = []  # Store mean rewards across agents\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        env.reset()\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        cumulative_rewards = {agent: 0 for agent in env.agents}  # Cumulative rewards per episode\n",
    "        \n",
    "        print(f\"Episode: {episode}\")\n",
    "\n",
    "        for agent in env.agents:\n",
    "            if terminated or truncated:\n",
    "                print(not (terminated or truncated))\n",
    "            current_agent = env.current_agent\n",
    "                \n",
    "            obs, _, terminated, truncated, _ = env.env.last()\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "                \n",
    "            action, _states = agents[agent].predict(obs, deterministic=True)\n",
    "            env.step(action)\n",
    "                \n",
    "            next_obs, reward, terminated, truncated, _ = env.env.last()\n",
    "                \n",
    "            agents[agent].learn(total_timesteps=1000)\n",
    "\n",
    "\n",
    "        # Optionally evaluate performance\n",
    "        #if episode % 10 == 0:\n",
    "        for agent in agents:\n",
    "            mean_reward, _ = evaluate_policy(agents[agent], wrapped_env, n_eval_episodes=10)\n",
    "            agent_rewards[agent].append(mean_reward)  # Record cumulative reward for each agent\n",
    "\n",
    "            print(f\"Agent {agent}: Mean reward: {mean_reward}\")\n",
    "\n",
    "        mean_rewards.append(np.mean([agent_rewards[agent] for agent in env.agents]))  # Record mean reward across agents\n",
    "\n",
    "    env.env.close()\n",
    "\n",
    "    return agent_rewards, mean_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to plot the learning curves for each agent and the mean reward\n",
    "def plot_learning_curves(agent_rewards, mean_rewards):\n",
    "    plt.figure(figsize=(14, 8))\n",
    "\n",
    "    # Plot the learning curve for each agent\n",
    "    plt.subplot(2, 1, 1)\n",
    "    for agent, rewards in agent_rewards.items():\n",
    "        plt.plot(rewards, label=f'Agent {agent}')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Cumulative Reward')\n",
    "    plt.title('Learning Curves for Each Agent')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot the mean reward curve\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(mean_rewards, label='Mean Reward', color='black', linewidth=2)\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Mean Reward')\n",
    "    plt.title('Mean Reward Across All Agents')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0\n",
      "Agent archer_0: Mean reward: -0.2\n",
      "Agent archer_1: Mean reward: 0.0\n",
      "Agent knight_0: Mean reward: 0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m agents \u001b[38;5;241m=\u001b[39m {agent: DQN(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m, wrapped_env, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhyperparams) \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m env\u001b[38;5;241m.\u001b[39magents}\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#agents = {agent: DQN(\"MlpPolicy\", wrapped_env, verbose=0) for agent in env.agents}\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m agent_rewards, mean_rewards \u001b[38;5;241m=\u001b[39m train_with_logging(wrapped_env, agents, num_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Plot the learning curves\u001b[39;00m\n\u001b[1;32m     20\u001b[0m plot_learning_curves(agent_rewards, mean_rewards)\n",
      "Cell \u001b[0;32mIn[10], line 33\u001b[0m, in \u001b[0;36mtrain_with_logging\u001b[0;34m(env, agents, num_episodes)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Optionally evaluate performance\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m#if episode % 10 == 0:\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m agents:\n\u001b[0;32m---> 33\u001b[0m     mean_reward, _ \u001b[38;5;241m=\u001b[39m evaluate_policy(agents[agent], wrapped_env, n_eval_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m     34\u001b[0m     agent_rewards[agent]\u001b[38;5;241m.\u001b[39mappend(mean_reward)  \u001b[38;5;66;03m# Record cumulative reward for each agent\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAgent \u001b[39m\u001b[38;5;132;01m{\u001b[39;00magent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Mean reward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean_reward\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/stable_baselines3/common/evaluation.py:94\u001b[0m, in \u001b[0;36mevaluate_policy\u001b[0;34m(model, env, n_eval_episodes, deterministic, render, callback, reward_threshold, return_episode_rewards, warn)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m (episode_counts \u001b[38;5;241m<\u001b[39m episode_count_targets)\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m     88\u001b[0m     actions, states \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(\n\u001b[1;32m     89\u001b[0m         observations,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m     90\u001b[0m         state\u001b[38;5;241m=\u001b[39mstates,\n\u001b[1;32m     91\u001b[0m         episode_start\u001b[38;5;241m=\u001b[39mepisode_starts,\n\u001b[1;32m     92\u001b[0m         deterministic\u001b[38;5;241m=\u001b[39mdeterministic,\n\u001b[1;32m     93\u001b[0m     )\n\u001b[0;32m---> 94\u001b[0m     new_observations, rewards, dones, infos \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(actions)\n\u001b[1;32m     95\u001b[0m     current_rewards \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m rewards\n\u001b[1;32m     96\u001b[0m     current_lengths \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:206\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_wait()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:58\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# Avoid circular imports\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[0;32m---> 58\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs[env_idx]\u001b[38;5;241m.\u001b[39mstep(\n\u001b[1;32m     59\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactions[env_idx]\n\u001b[1;32m     60\u001b[0m         )\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;66;03m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx] \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "Cell \u001b[0;32mIn[8], line 18\u001b[0m, in \u001b[0;36mPettingZooEnvWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     16\u001b[0m y_tolerance \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.98\u001b[39m\n\u001b[1;32m     17\u001b[0m collision_threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.02\u001b[39m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     19\u001b[0m num_zombies \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mmax_zombies\n\u001b[1;32m     20\u001b[0m obs, reward,terminated,truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mlast()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pettingzoo/utils/wrappers/order_enforcing.py:96\u001b[0m, in \u001b[0;36mOrderEnforcingWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pettingzoo/utils/wrappers/base.py:47\u001b[0m, in \u001b[0;36mBaseWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action: ActionType) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pettingzoo/utils/wrappers/assert_out_of_bounds.py:26\u001b[0m, in \u001b[0;36mAssertOutOfBoundsWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action: ActionType) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m     18\u001b[0m         action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m         action\n\u001b[1;32m     25\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maction is not in action space\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 26\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pettingzoo/utils/wrappers/base.py:47\u001b[0m, in \u001b[0;36mBaseWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action: ActionType) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pettingzoo/butterfly/knights_archers_zombies/knights_archers_zombies.py:738\u001b[0m, in \u001b[0;36mraw_env.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspawn_zombie()\n\u001b[1;32m    737\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscreen \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 738\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdraw()\n\u001b[1;32m    740\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_game_end()\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframes \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pettingzoo/butterfly/knights_archers_zombies/knights_archers_zombies.py:783\u001b[0m, in \u001b[0;36mraw_env.draw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscreen\u001b[38;5;241m.\u001b[39mfill((\u001b[38;5;241m66\u001b[39m, \u001b[38;5;241m40\u001b[39m, \u001b[38;5;241m53\u001b[39m))\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscreen\u001b[38;5;241m.\u001b[39mblit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft_wall, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft_wall\u001b[38;5;241m.\u001b[39mget_rect())\n\u001b[0;32m--> 783\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscreen\u001b[38;5;241m.\u001b[39mblit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright_wall, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright_wall_rect)\n\u001b[1;32m    784\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscreen\u001b[38;5;241m.\u001b[39mblit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfloor_patch1, (\u001b[38;5;241m500\u001b[39m, \u001b[38;5;241m500\u001b[39m))\n\u001b[1;32m    785\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscreen\u001b[38;5;241m.\u001b[39mblit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfloor_patch2, (\u001b[38;5;241m900\u001b[39m, \u001b[38;5;241m30\u001b[39m))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = knights_archers_zombies_v10.env(render_mode=\"rgb_array\",max_zombies=4,max_cycles=100)\n",
    "wrapped_env = PettingZooEnvWrapper(env)\n",
    "\n",
    "hyperparams={'gamma': 0.99,\n",
    " 'learning_rate': 0.0001774658772353578,\n",
    " 'batch_size': 32,\n",
    " 'buffer_size': 10000,\n",
    " 'exploration_final_eps': 0.08646173611211427,\n",
    " 'exploration_fraction': 0.32951412388518647,\n",
    " 'target_update_interval': 1000,\n",
    " 'learning_starts': 0,\n",
    " 'train_freq': 128,\n",
    " \"policy_kwargs\": dict(net_arch=[64])}\n",
    "\n",
    "agents = {agent: DQN(\"MlpPolicy\", wrapped_env, verbose=0,**hyperparams) for agent in env.agents}\n",
    "#agents = {agent: DQN(\"MlpPolicy\", wrapped_env, verbose=0) for agent in env.agents}\n",
    "agent_rewards, mean_rewards = train_with_logging(wrapped_env, agents, num_episodes=1000)\n",
    "\n",
    "# Plot the learning curves\n",
    "plot_learning_curves(agent_rewards, mean_rewards)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
