{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-05 16:53:56.013913: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-05 16:53:56.030198: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-05 16:53:56.051852: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-05 16:53:56.058247: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-05 16:53:56.075018: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-05 16:53:57.304426: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from pettingzoo.butterfly import knights_archers_zombies_v10\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import optuna\n",
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L'environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PettingZooEnvWrapper(gym.Env):\n",
    "    def __init__(self, env):\n",
    "        super(PettingZooEnvWrapper, self).__init__()\n",
    "        self.env = env\n",
    "        self.env.reset()\n",
    "        self.action_space = gym.spaces.Discrete(self.env.action_space(self.env.agents[0]).n)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=1, shape=self.env.observation_space(self.env.agents[0]).shape, dtype=np.float32)\n",
    "        self.current_agent = self.env.agents[0]\n",
    "        self.agents=self.env.agents\n",
    "    def reset(self,seed=None, **kwargs):\n",
    "        self.env.reset(seed=seed)\n",
    "        self.current_agent = self.env.agents[0]\n",
    "        obs, _, _, _,infos = self.env.last()\n",
    "        return obs,infos\n",
    "    def step(self, action):\n",
    "        self.env.step(action)\n",
    "        obs, reward,terminated,truncated, info = self.env.last()\n",
    "        self.current_agent = self.env.agent_selection\n",
    "        return obs, reward, terminated,truncated, info\n",
    "    \n",
    "    def render(self):\n",
    "        return self.env.render()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env = knights_archers_zombies_v10.env(render_mode=\"human\",max_zombies=4,max_cycles=100)\n",
    "env = knights_archers_zombies_v10.env(render_mode=\"rgb_array\",max_zombies=4,max_cycles=100)\n",
    "\n",
    "wrapped_env = PettingZooEnvWrapper(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boucle d'apprentisage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, agents, num_episodes=100,affichage=True):\n",
    "    for episode in range(num_episodes):\n",
    "        env.reset()\n",
    "        terminated = False\n",
    "        truncated=False\n",
    "        #print(episode)\n",
    "        for agent in env.agents:\n",
    "            if terminated or truncated:\n",
    "                print(not (terminated or truncated))\n",
    "            current_agent = env.current_agent\n",
    "                \n",
    "            obs, _, terminated,truncated, _ = env.env.last()\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "                \n",
    "            action, _states = agents[agent].predict(obs, deterministic=True)\n",
    "            #print(action)\n",
    "            env.step(action)\n",
    "                \n",
    "            next_obs, reward, terminated,truncated, _ = env.env.last()\n",
    "                \n",
    "            # Collect experience and train the agent\n",
    "            agents[agent].learn(total_timesteps=1000)\n",
    "\n",
    "                \n",
    "        # Optionally evaluate performance\n",
    "        #if episode % 10 == 0 and affichage:\n",
    "        for agent in agents:\n",
    "            mean_reward, _ = evaluate_policy(agents[agent], wrapped_env, n_eval_episodes=10)\n",
    "            print(f\"Agent {agent}: Mean reward: {mean_reward}\")\n",
    "\n",
    "    env.env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optuna optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_dqn_params(trial: optuna.Trial) :\n",
    "    \"\"\"\n",
    "    Sampler for DQN hyperparams.\n",
    "\n",
    "    :param trial:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    gamma = trial.suggest_categorical(\"gamma\", [0.9, 0.95, 0.98, 0.99, 0.995, 0.999, 0.9999])\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64, 100, 128, 256, 512])\n",
    "    buffer_size = trial.suggest_categorical(\"buffer_size\", [int(1e4), int(5e4), int(1e5), int(1e6)])\n",
    "    exploration_final_eps = trial.suggest_float(\"exploration_final_eps\", 0, 0.2)\n",
    "    exploration_fraction = trial.suggest_float(\"exploration_fraction\", 0, 0.5)\n",
    "    target_update_interval = trial.suggest_categorical(\"target_update_interval\", [1, 1000, 5000, 10000, 15000, 20000])\n",
    "    learning_starts = trial.suggest_categorical(\"learning_starts\", [0, 1000, 5000, 10000, 20000])\n",
    "\n",
    "    train_freq = trial.suggest_categorical(\"train_freq\", [1, 4, 8, 16, 128, 256, 1000])\n",
    "    subsample_steps = trial.suggest_categorical(\"subsample_steps\", [1, 2, 4, 8])\n",
    "    gradient_steps = max(train_freq // subsample_steps, 1)\n",
    "\n",
    "    net_arch_type = trial.suggest_categorical(\"net_arch\", [\"tiny\", \"small\", \"medium\"])\n",
    "\n",
    "    net_arch = {\"tiny\": [64], \"small\": [64, 64], \"medium\": [256, 256]}[net_arch_type]\n",
    "\n",
    "    hyperparams = {\n",
    "        \"gamma\": gamma,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"buffer_size\": buffer_size,\n",
    "        \"train_freq\": train_freq,\n",
    "        \"gradient_steps\": gradient_steps,\n",
    "        \"exploration_fraction\": exploration_fraction,\n",
    "        \"exploration_final_eps\": exploration_final_eps,\n",
    "        \"target_update_interval\": target_update_interval,\n",
    "        \"learning_starts\": learning_starts,\n",
    "        \"policy_kwargs\": dict(net_arch=net_arch),\n",
    "    }\n",
    "\n",
    "    env = knights_archers_zombies_v10.env(render_mode=\"rgb_array\",max_zombies=4,max_cycles=100)\n",
    "\n",
    "    wrapped_env = PettingZooEnvWrapper(env)\n",
    "    agents = {agent: DQN(\"MlpPolicy\", wrapped_env, verbose=0,**hyperparams) for agent in env.agents}\n",
    "\n",
    "    train(wrapped_env,agents,num_episodes=10,affichage=False)\n",
    "    total_reward=0\n",
    "    for agent in agents:\n",
    "            mean_reward, _ = evaluate_policy(agents[agent], wrapped_env, n_eval_episodes=10)\n",
    "            total_reward+=mean_reward\n",
    "\n",
    "\n",
    "    return -total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_dqn=optuna.study.create_study()\n",
    "study_dqn.optimize(sample_dqn_params, n_trials=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hyperparams={'gamma': 0.95,\n",
    " 'learning_rate': 0.0057832857004646385,\n",
    " 'batch_size': 512,\n",
    " 'buffer_size': 1000000,\n",
    " 'exploration_final_eps': 0.038076390363142434,\n",
    " 'exploration_fraction': 0.1311871993781526,\n",
    " 'target_update_interval': 15000,\n",
    " 'learning_starts': 1000,\n",
    " 'train_freq': 256,\n",
    " \"policy_kwargs\": dict(net_arch=[64])}\n",
    "agents = {agent: DQN(\"MlpPolicy\", wrapped_env, verbose=0,**best_hyperparams) for agent in env.agents}\n",
    "#best_hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(wrapped_env, agents, 5) #10 pas d'apprentissage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affichage de l'environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import imageio\n",
    "\n",
    "# Configurer le répertoire pour enregistrer la vidéo\n",
    "video_directory = \"./vids\"\n",
    "os.makedirs(video_directory, exist_ok=True)\n",
    "video_path = os.path.join(video_directory, \"trained_agents.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_trained_agents_with_custom_video(env, agents, max_steps_per_episode=100):\n",
    "    obs, _ = env.reset()  # Reset environment\n",
    "    step_count = 0  # Initialize step counter\n",
    "    frames = []  # Liste pour stocker les frames de l'épisode\n",
    "\n",
    "    while step_count < max_steps_per_episode:\n",
    "        current_agent = env.current_agent\n",
    "\n",
    "        # Get current observation\n",
    "        obs, _, _, _, _ = env.env.last()\n",
    "\n",
    "        # Get action from the trained agent's policy\n",
    "        action, _states = agents[current_agent].predict(obs, deterministic=True)\n",
    "\n",
    "        # Step through the environment\n",
    "        next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "        # Render the environment and append the frame to frames list\n",
    "        frame = env.render()\n",
    "        frames.append(frame)  # Ajouter la frame pour la vidéo\n",
    "\n",
    "        # Increment the step counter\n",
    "        step_count += 1\n",
    "\n",
    "        # Terminate if the episode is done\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    env.env.close()  # Close the environment\n",
    "\n",
    "    # Enregistrer la vidéo avec imageio\n",
    "    imageio.mimsave(video_path, frames, fps=30)\n",
    "    print(f\"Video saved in: {video_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved in: ./vids/trained_agents.mp4\n"
     ]
    }
   ],
   "source": [
    "# human_env = knights_archers_zombies_v10.env(render_mode=\"rgb_array\", max_zombies=4, max_cycles=1000)\n",
    "\n",
    "# human_wrapped_env = PettingZooEnvWrapper(human_env)\n",
    "\n",
    "render_trained_agents_with_custom_video(wrapped_env, agents, max_steps_per_episode=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_trained_agents(env, agents, max_steps_per_episode=100):\n",
    "    obs, _ = env.reset()  # Reset environment\n",
    "    step_count = 0  # Initialize step counter\n",
    "\n",
    "    while step_count < max_steps_per_episode:\n",
    "        current_agent = env.current_agent\n",
    "\n",
    "        # Get current observation\n",
    "        obs, _, _, _, _ = env.env.last()\n",
    "\n",
    "        # Get action from the trained agent's policy\n",
    "        action, _states = agents[current_agent].predict(obs, deterministic=True)\n",
    "\n",
    "        # Step through the environment\n",
    "        next_obs, reward, _, _, _ = env.step(action)\n",
    "\n",
    "        # Render the environment\n",
    "        env.render()\n",
    "\n",
    "        # Increment the step counter\n",
    "        step_count += 1\n",
    "\n",
    "    env.env.close()  # Close the environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_env = knights_archers_zombies_v10.env(render_mode=\"human\",max_zombies=4,max_cycles=1000)\n",
    "\n",
    "human_wrapped_env = PettingZooEnvWrapper(human_env)\n",
    "render_trained_agents(human_wrapped_env, agents, max_steps_per_episode=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot de performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_logging(env, agents, num_episodes=10000):\n",
    "    agent_rewards = {agent: [] for agent in env.agents}  # Store cumulative rewards for each agent\n",
    "    mean_rewards = []  # Store mean rewards across agents\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        env.reset()\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        cumulative_rewards = {agent: 0 for agent in env.agents}  # Cumulative rewards per episode\n",
    "        \n",
    "        print(f\"Episode: {episode}\")\n",
    "\n",
    "        for agent in env.agents:\n",
    "            if terminated or truncated:\n",
    "                print(not (terminated or truncated))\n",
    "            current_agent = env.current_agent\n",
    "                \n",
    "            obs, _, terminated, truncated, _ = env.env.last()\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "                \n",
    "            action, _states = agents[agent].predict(obs, deterministic=True)\n",
    "            env.step(action)\n",
    "                \n",
    "            next_obs, reward, terminated, truncated, _ = env.env.last()\n",
    "                \n",
    "            agents[agent].learn(total_timesteps=1000)\n",
    "\n",
    "\n",
    "        # Optionally evaluate performance\n",
    "        #if episode % 10 == 0:\n",
    "        for agent in agents:\n",
    "            mean_reward, _ = evaluate_policy(agents[agent], wrapped_env, n_eval_episodes=10)\n",
    "            agent_rewards[agent].append(mean_reward)  # Record cumulative reward for each agent\n",
    "\n",
    "            print(f\"Agent {agent}: Mean reward: {mean_reward}\")\n",
    "\n",
    "        mean_rewards.append(np.mean([agent_rewards[agent] for agent in env.agents]))  # Record mean reward across agents\n",
    "\n",
    "    env.env.close()\n",
    "\n",
    "    return agent_rewards, mean_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to plot the learning curves for each agent and the mean reward\n",
    "def plot_learning_curves(agent_rewards, mean_rewards):\n",
    "    plt.figure(figsize=(14, 8))\n",
    "\n",
    "    # Plot the learning curve for each agent\n",
    "    plt.subplot(2, 1, 1)\n",
    "    for agent, rewards in agent_rewards.items():\n",
    "        plt.plot(rewards, label=f'Agent {agent}')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Cumulative Reward')\n",
    "    plt.title('Learning Curves for Each Agent')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot the mean reward curve\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(mean_rewards, label='Mean Reward', color='black', linewidth=2)\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Mean Reward')\n",
    "    plt.title('Mean Reward Across All Agents')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = knights_archers_zombies_v10.env(render_mode=\"rgb_array\",max_zombies=4,max_cycles=100)\n",
    "wrapped_env = PettingZooEnvWrapper(env)\n",
    "\n",
    "hyperparams={'gamma': 0.99,\n",
    " 'learning_rate': 0.0001774658772353578,\n",
    " 'batch_size': 32,\n",
    " 'buffer_size': 10000,\n",
    " 'exploration_final_eps': 0.08646173611211427,\n",
    " 'exploration_fraction': 0.32951412388518647,\n",
    " 'target_update_interval': 1000,\n",
    " 'learning_starts': 0,\n",
    " 'train_freq': 128,\n",
    " \"policy_kwargs\": dict(net_arch=[64])}\n",
    "\n",
    "#agents = {agent: DQN(\"MlpPolicy\", wrapped_env, verbose=0,**hyperparams) for agent in env.agents}\n",
    "agents = {agent: DQN(\"MlpPolicy\", wrapped_env, verbose=0) for agent in env.agents}\n",
    "#agent_rewards, mean_rewards = train_with_logging(wrapped_env, agents, num_episodes=1000)\n",
    "agent_rewards, mean_rewards = train_with_logging(wrapped_env, agents, num_episodes=5)\n",
    "\n",
    "# Plot the learning curves\n",
    "plot_learning_curves(agent_rewards, mean_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_directory = \"./vids\"\n",
    "os.makedirs(video_directory, exist_ok=True)\n",
    "video_path = os.path.join(video_directory, \"trained_agents_1000_episodes_dumb.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved in: ./vids/trained_agents_1000_episodes_dumb.mp4\n"
     ]
    }
   ],
   "source": [
    "# human_env = knights_archers_zombies_v10.env(render_mode=\"rgb_array\", max_zombies=4, max_cycles=1000)\n",
    "\n",
    "# human_wrapped_env = PettingZooEnvWrapper(human_env)\n",
    "\n",
    "render_trained_agents_with_custom_video(wrapped_env, agents, max_steps_per_episode=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
